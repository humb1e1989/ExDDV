{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eduahoge\\AppData\\Local\\miniconda3\\envs\\llava\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\eduahoge\\AppData\\Local\\miniconda3\\envs\\llava\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\eduahoge\\AppData\\Local\\miniconda3\\envs\\llava\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\eduahoge\\AppData\\Local\\miniconda3\\envs\\llava\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique videos: 816\n",
      "cuda\n",
      "CLIP Model Architecture:\n",
      "Now running 2nd prompt, resnet101, middle layer\n",
      "Total number of videos before filtering: 100\n",
      "Total rows in dataset: 100\n",
      "Total rows to be processed: 100\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\304_M019.mp4: 6\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\hoaweiathp.mp4: 6\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_48\\ghwlogkoic.mp4: 4\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\443_M137.mp4: 13\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_2\\yirhsptlko.mp4: 4\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_2\\uqvxjfpwdo.mp4: 3\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\384_M027.mp4: 6\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\311_M135.mp4: 12\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\flnhfqfhsk.mp4: 3\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\312_W101.mp4: 12\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\486_W031.mp4: 5\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\095_W017.mp4: 6\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_2\\saokspyemj.mp4: 1\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\iuttprjxhf.mp4: 3\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\287_W125.mp4: 10\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\348_W131.mp4: 9\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\368_W005.mp4: 8\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_48\\mjlqcikpbb.mp4: 5\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_48\\luhndqsljl.mp4: 5\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\ltdwtgrads.mp4: 2\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\299_M105.mp4: 9\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\160_M005.mp4: 4\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_48\\kisvmxjmeb.mp4: 5\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\256_W042.mp4: 8\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\166_W028.mp4: 5\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\306_M019.mp4: 4\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\259_M014.mp4: 13\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\451_M138.mp4: 11\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_48\\aofjdfbvsf.mp4: 8\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\004_M101.mp4: 7\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\mpkmkclffv.mp4: 9\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_48\\bvylomlyys.mp4: 4\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\avtdwylihb.mp4: 5\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\848_M021.mp4: 3\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\378_W014.mp4: 4\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\874_M025.mp4: 3\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\gicqpeyqxs.mp4: 10\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\025_W007.mp4: 5\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\065_W005.mp4: 6\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\328_W008.mp4: 7\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\940_W100.mp4: 7\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\165_W028.mp4: 12\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_48\\lhurhgpffk.mp4: 3\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\410_W019.mp4: 17\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\326_M021.mp4: 7\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\033_M114.mp4: 9\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\307_M019.mp4: 11\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\363_M025.mp4: 7\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_48\\ciepqxjamz.mp4: 4\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\hjblygobtf.mp4: 9\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\407_M029.mp4: 10\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\cfmiliboth.mp4: 11\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\716_W019.mp4: 10\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\081_M120.mp4: 5\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\862_W039.mp4: 3\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_48\\kigbymagcu.mp4: 9\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\569_M115.mp4: 4\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\470_M139.mp4: 11\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\281_W116.mp4: 6\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\492_M038.mp4: 6\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\118_W132.mp4: 12\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\kxqpyldfzj.mp4: 10\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\cbxtulzkxr.mp4: 11\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\035_M114.mp4: 10\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\283_M134.mp4: 9\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\091_W017.mp4: 7\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\465_W026.mp4: 10\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\309_W101.mp4: 6\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\czlanpxaoq.mp4: 5\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\039_M114.mp4: 4\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\ctzvhowbhr.mp4: 8\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\398_W017.mp4: 6\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\008_W101.mp4: 7\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\335_M022.mp4: 8\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\087_M120.mp4: 6\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\hmqcnrnldb.mp4: 17\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\228_M011.mp4: 6\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\489_M038.mp4: 4\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\041_M115.mp4: 12\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_48\\ixcrtusrfi.mp4: 7\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_48\\alzajhomle.mp4: 2\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\175_M133.mp4: 14\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_48\\aafuphyhpt.mp4: 9\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\903_M029.mp4: 1\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_0\\nwvloufjty.mp4: 3\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\430_W022.mp4: 13\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_0\\ybefzibreb.mp4: 1\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_48\\bfawveelwz.mp4: 7\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\432_M136.mp4: 13\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\mvathrqhbe.mp4: 4\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\089_W016.mp4: 5\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\howruupkxw.mp4: 7\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\395_M028.mp4: 6\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\dfdc_train_part_49\\jruqdkdnnz.mp4: 8\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\036_M114.mp4: 3\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\779_M013.mp4: 4\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\409_M030.mp4: 13\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\457_W025.mp4: 8\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\265_W111.mp4: 12\n",
      "Number of relevant frames for video C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\191_W032.mp4: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing validation frames:   0%|          | 0/100 [00:00<?, ?it/s]c:\\Users\\eduahoge\\AppData\\Local\\miniconda3\\envs\\llava\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Analyzing validation frames: 100%|██████████| 100/100 [4:48:34<00:00, 173.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to similarity_results_clip_first_resnet101.csv\n",
      "Most Similar Pair:\n",
      "Validation Video: C:\\transf\\XAI_DEEPFAKE\\LLaVA\\end_to_end\\304_M019.mp4 at frame 345\n",
      "Cosine Similarity (Highest): 0.0000\n",
      "\n",
      "Annotation for the Closest Training Frame:\n",
      "Light flickering on face.\n",
      "Deepfake Analysis for Validation Frame:\n",
      "The face in the image is described as having a light flickering on it. However, there are no deepfake artifacts or manipulations visible in the image. The lighting in the room might be causing a reflection on the man's face, but it is not a deepfake or manipulation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('LLaVA')\n",
    "sys.path.append('C:\\transf\\XAI_DEEPFAKE\\LLaVA')\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from pathlib import Path\n",
    "import json\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model_frame\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# if this happens, restart the notebook\n",
    "def safe_load_model(model_path, max_retries=3, delay=5):\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            tokenizer, llava_model, image_processor, context_len = load_pretrained_model(\n",
    "                model_path=model_path,\n",
    "                model_base=None,\n",
    "                model_name=get_model_name_from_path(model_path)\n",
    "            )\n",
    "            return tokenizer, llava_model, image_processor, context_len\n",
    "        except ZeroDivisionError:\n",
    "            print(f\"ZeroDivisionError during model loading. Retrying... (Attempt {attempt + 1})\")\n",
    "            time.sleep(delay)\n",
    "            attempt += 1\n",
    "\n",
    "    raise RuntimeError(\"Failed to load the model after multiple attempts\")\n",
    "\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "tokenizer, llava_model, image_processor, context_len = safe_load_model(model_path)\n",
    "\n",
    "llava_model = llava_model.to(device)\n",
    "\n",
    "csv_path = r'C:\\transf\\XAI_DEEPFAKE\\dataset.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "unique_videos = df['movie_name'].nunique()\n",
    "print(f\"Total unique videos: {unique_videos}\")\n",
    "\n",
    "# Split dataset into training and validation sets at the video level\n",
    "df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "video_paths = df['movie_name'].unique()\n",
    "val_videos = video_paths[:100]\n",
    "train_videos = video_paths[100:]\n",
    "train_df = df[df['movie_name'].isin(train_videos)]\n",
    "val_df = df[df['movie_name'].isin(val_videos)]\n",
    "\n",
    "# Load CLIP model for extracting embeddings\n",
    "print(device)\n",
    "clip_model, preprocess = clip.load(\"RN50\", device=device)  # Using ResNet-50 backbone, can be changed to RN101\n",
    "clip_model.eval()\n",
    "\n",
    "print(\"CLIP Model Architecture:\")\n",
    "#print(clip_model)\n",
    "\n",
    "# Function to get specific layer embedding from CLIP\n",
    "def get_clip_embedding(image, layer=\"last\"):\n",
    "    image = preprocess(image).unsqueeze(0).to(device)\n",
    "    # Ensure input type matches model weights\n",
    "    image = image.half() if clip_model.visual.conv1.weight.dtype == torch.half else image\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if layer == \"first\":\n",
    "            #print(\"Extracting features after first block...\")\n",
    "            x = clip_model.visual.conv1(image)\n",
    "            #print(f\"Shape after conv1: {x.shape}\")\n",
    "            x = clip_model.visual.bn1(x)\n",
    "            x = clip_model.visual.relu1(x)\n",
    "            x = clip_model.visual.conv2(x)\n",
    "            #print(f\"Shape after conv2: {x.shape}\")\n",
    "            x = clip_model.visual.bn2(x)\n",
    "            x = clip_model.visual.relu2(x)\n",
    "            x = clip_model.visual.conv3(x)\n",
    "            #print(f\"Shape after conv3: {x.shape}\")\n",
    "            x = clip_model.visual.bn3(x)\n",
    "            x = clip_model.visual.relu3(x)\n",
    "            x = clip_model.visual.avgpool(x)\n",
    "            #print(f\"Shape after avgpool: {x.shape}\")\n",
    "            embedding = x\n",
    "\n",
    "\n",
    "        elif layer == \"middle\":\n",
    "            x = clip_model.visual.conv1(image)\n",
    "            x = clip_model.visual.bn1(x)\n",
    "            x = clip_model.visual.relu1(x)\n",
    "            x = clip_model.visual.conv2(x)\n",
    "            x = clip_model.visual.bn2(x)\n",
    "            x = clip_model.visual.relu2(x)\n",
    "            x = clip_model.visual.conv3(x)\n",
    "            x = clip_model.visual.bn3(x)\n",
    "            x = clip_model.visual.relu3(x)\n",
    "            x = clip_model.visual.avgpool(x)  # Apply average pooling\n",
    "\n",
    "            x = clip_model.visual.layer1(x)\n",
    "            x = clip_model.visual.layer2(x)\n",
    "\n",
    "            embedding = x\n",
    "\n",
    "        elif layer == \"last\":\n",
    "            embedding = clip_model.encode_image(image)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layer specified. Choose from 'first', 'middle', or 'last'.\")\n",
    "\n",
    "        # Flatten the embedding to ensure consistent shape across layers\n",
    "        embedding = embedding.view(embedding.size(0), -1)  # Flatten spatial dimensions\n",
    "    return embedding\n",
    "\n",
    "# logic to detect deepfake artifacts in a frame\n",
    "def detect_deepfake(frame, custom_prompt):\n",
    "    if isinstance(frame, np.ndarray):\n",
    "        frame = frame.astype('uint8')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid frame data. Expected a NumPy array.\")\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_pil = Image.fromarray(frame_rgb)\n",
    "    \n",
    "    local_args = type('Args', (), {\n",
    "        \"model_path\": model_path,\n",
    "        \"model_base\": None,\n",
    "        \"model_name\": get_model_name_from_path(model_path),\n",
    "        \"query\": custom_prompt,\n",
    "        \"conv_mode\": None,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": None,\n",
    "        \"num_beams\": 1,\n",
    "        \"max_new_tokens\": 512,\n",
    "    })()\n",
    "    \n",
    "    predicted_sentence = eval_model_frame(local_args, image_pil, tokenizer=tokenizer, model=llava_model, image_processor=image_processor)\n",
    "    return predicted_sentence\n",
    "\n",
    "def create_custom_prompt(annotation):\n",
    "    base_prompt = (\n",
    "        \"Based on the following description: '{annotation}', analyze the face in the image and identify any signs of deepfake artifacts. \"\n",
    "        \"Consider how the description might relate to potential manipulations. \"\n",
    "        \"Look for abnormalities such as inconsistent lighting, unnatural facial movements, \"\n",
    "        \"blurriness around the edges, strange reflections in the eyes, mismatched facial features, \"\n",
    "        \"or any other indications of digital manipulation. Provide a detailed description of any \"\n",
    "        \"anomalies found.\"\n",
    "    )\n",
    "    return base_prompt.format(annotation=annotation)\n",
    "\n",
    "# def create_custom_prompt(annotation):\n",
    "#     base_prompt = (\n",
    "#         \"Analyze the face in the image based on the description: '{annotation}'. Identify any deepfake artifacts, focusing specifically on the \"\n",
    "#         \"affected parts of the face mentioned. Provide a short and direct explanation highlighting the inconsistencies or manipulations.\"\n",
    "#     )\n",
    "#     return base_prompt.format(annotation=annotation)\n",
    "\n",
    "# Extract relevant information from the CSV\n",
    "def get_relevant_frames_from_csv(df, num_files=0):\n",
    "    print(f\"Total number of videos before filtering: {df['movie_name'].nunique()}\")\n",
    "    print(f\"Total rows in dataset: {len(df)}\")\n",
    "\n",
    "    frames = {}\n",
    "    total_rows = len(df)\n",
    "    print(f\"Total rows to be processed: {total_rows}\")\n",
    "    if num_files > 0:\n",
    "        total_rows = min(total_rows, num_files)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if num_files > 0 and idx >= num_files:\n",
    "            break\n",
    "        \n",
    "        video_name = row['movie_name']\n",
    "        manipulation_path = Path('C:/transf/XAI_DEEPFAKE/LLaVA') / row['manipulation']\n",
    "        video_path = (manipulation_path / video_name).resolve()\n",
    "        click_locations = row['click_locations']\n",
    "        annotation_text = row['text']\n",
    "        \n",
    "        if not click_locations or pd.isna(click_locations) or not annotation_text or pd.isna(annotation_text):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            frame_data = json.loads(click_locations)\n",
    "            relevant_frames = [int(frame) - 1 for frame in frame_data.keys() if frame.isdigit()]\n",
    "            frames[video_path] = relevant_frames\n",
    "            print(f\"Number of relevant frames for video {video_path}: {len(relevant_frames)}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    return frames\n",
    "\n",
    "# Main function\n",
    "def main(display=True, save=True, num_files=0, layer=\"last\"):\n",
    "    frames = get_relevant_frames_from_csv(val_df, num_files)  # Process only validation videos\n",
    "    train_embeddings = {}\n",
    "\n",
    "    # Encode frames from training set only\n",
    "    for idx, row in train_df.iterrows():\n",
    "        video_name = row['movie_name']\n",
    "        manipulation_path = Path('C:/transf/XAI_DEEPFAKE/LLaVA') / row['manipulation']\n",
    "        video_path = (manipulation_path / video_name).resolve()\n",
    "        click_locations = row['click_locations']\n",
    "\n",
    "        if not click_locations or pd.isna(click_locations):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            frame_data = json.loads(click_locations)\n",
    "            relevant_frames = [int(frame) - 1 for frame in frame_data.keys() if frame.isdigit()]\n",
    "            for frame_number in relevant_frames:\n",
    "                cap = cv2.VideoCapture(str(video_path))\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    image_pil = Image.fromarray(frame_rgb)\n",
    "                    embedding = get_clip_embedding(image_pil, layer=layer)\n",
    "                    train_embeddings[(video_path, frame_number)] = (embedding, row['text'])\n",
    "                cap.release()\n",
    "        except json.JSONDecodeError as e:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # Analyze validation frames\n",
    "    results = []\n",
    "    for video_path, frame_list in tqdm(frames.items(), desc=\"Analyzing validation frames\"):\n",
    "        for frame_number in frame_list:\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                cap.release()\n",
    "                continue\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image_pil = Image.fromarray(frame_rgb)\n",
    "            val_embedding = get_clip_embedding(image_pil, layer=layer)\n",
    "            cap.release()\n",
    "\n",
    "            # Find the most similar frame in the training set\n",
    "            max_similarity = -1\n",
    "            best_annotation = None\n",
    "            for (train_video_path, train_frame_number), (train_embedding, annotation) in train_embeddings.items():\n",
    "                similarity = cosine_similarity(val_embedding, train_embedding).item()\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_annotation = annotation\n",
    "\n",
    "            # Create custom prompt using the closest training frame's annotation\n",
    "            custom_prompt = create_custom_prompt(best_annotation) if best_annotation else \"No annotation available\"\n",
    "\n",
    "            # Analyze the validation frame using the custom prompt\n",
    "            test_deepfake_analysis = detect_deepfake(frame, custom_prompt) if best_annotation else \"No analysis available\"\n",
    "\n",
    "            # Store the result\n",
    "            results.append({\n",
    "                'validation_video': str(video_path),\n",
    "                'validation_frame': frame_number,\n",
    "                'closest_train_annotation': best_annotation,\n",
    "                'test_deepfake_analysis': test_deepfake_analysis,\n",
    "                'cosine_similarity': max_similarity,\n",
    "            })\n",
    "\n",
    "    # save results to CSV\n",
    "    if save:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_csv_path = 'similarity_results_clip_first_resnet101.csv'\n",
    "        results_df.to_csv(results_csv_path, index=False)\n",
    "        print(f\"Results saved to {results_csv_path}\")\n",
    "\n",
    "    #  display logs for the most similar frames\n",
    "    if display and results:\n",
    "        highest_similarity_result = max(results, key=lambda x: x['cosine_similarity'])\n",
    "        print(f\"Most Similar Pair:\")\n",
    "        print(f\"Validation Video: {highest_similarity_result['validation_video']} at frame {highest_similarity_result['validation_frame']}\")\n",
    "        print(f\"Cosine Similarity (Highest): {highest_similarity_result['cosine_similarity']:.4f}\\n\")\n",
    "        print(f\"Annotation for the Closest Training Frame:\")\n",
    "        print(highest_similarity_result['closest_train_annotation'])\n",
    "        print(f\"Deepfake Analysis for Validation Frame:\")\n",
    "        print(highest_similarity_result['test_deepfake_analysis'])\n",
    "    elif not results:\n",
    "        print(\"Could not find the most similar pair.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Now running 1sr prompt, resnet50, middle layer\")\n",
    "\n",
    "    main(display=True, save=True, num_files=0, layer=\"middle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset with GT column saved to C:\\transf\\XAI_DEEPFAKE\\LLaVA\\llava\\similarity_results_clip_first_resnet101_merged.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dataset_path = r'C:\\transf\\XAI_DEEPFAKE\\dataset.csv'\n",
    "similarity_results_path = r'C:\\transf\\XAI_DEEPFAKE\\LLaVA\\llava\\similarity_results_clip_first_resnet101.csv'\n",
    "\n",
    "video_annotations = pd.read_csv(dataset_path)\n",
    "similarity_results = pd.read_csv(similarity_results_path)\n",
    "\n",
    "video_annotations = video_annotations[['movie_name', 'text']]\n",
    "\n",
    "# Remove duplicates to ensure unique movie_name for mapping\n",
    "unique_video_annotations = video_annotations.drop_duplicates(subset='movie_name')\n",
    "\n",
    "# Extract movie names from validation_video paths in the second dataset\n",
    "similarity_results['movie_name'] = similarity_results['validation_video'].apply(lambda x: os.path.basename(x))\n",
    "\n",
    "# Map the GT column using the unique video annotations\n",
    "similarity_results['GT'] = similarity_results['movie_name'].map(\n",
    "    unique_video_annotations.set_index('movie_name')['text']\n",
    ")\n",
    "\n",
    "output_path = r'C:\\transf\\XAI_DEEPFAKE\\LLaVA\\llava\\similarity_results_clip_first_resnet101_merged.csv'\n",
    "similarity_results.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Updated dataset with GT column saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity (BERT): 0.6026665626833675\n",
      "Average Cosine Similarity (Sentence BERT): 0.4046939964821095\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "file_path = r'C:\\transf\\XAI_DEEPFAKE\\LLaVA\\llava\\similarity_results_clip_first_resnet101_merged.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Group data by validation video ID\n",
    "video_groups = df.groupby('validation_video')\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained Sentence BERT model\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    # Use the average of token embeddings (excluding padding tokens) for the embedding\n",
    "    token_embeddings = outputs.last_hidden_state  # Shape: (batch_size, seq_length, hidden_size)\n",
    "    attention_mask = inputs['attention_mask'].unsqueeze(-1)  # Shape: (batch_size, seq_length, 1)\n",
    "    masked_embeddings = token_embeddings * attention_mask\n",
    "    sum_embeddings = masked_embeddings.sum(dim=1)  # Shape: (batch_size, hidden_size)\n",
    "    valid_token_count = attention_mask.sum(dim=1)  # Shape: (batch_size, 1)\n",
    "    average_embedding = sum_embeddings / valid_token_count  # Shape: (batch_size, hidden_size)\n",
    "    return average_embedding.squeeze(0).numpy()\n",
    "\n",
    "# Function to get Sentence BERT embeddings\n",
    "def get_sbert_embedding(texts):\n",
    "    return sentence_model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "# Function to calculate cosine similarity between embeddings\n",
    "def calculate_cosine_similarity(embedding1, embedding2):\n",
    "    return cosine_similarity(embedding1, embedding2)[0][0]\n",
    "\n",
    "# Function to calculate average cosine similarity per video\n",
    "def calculate_average_cosine_similarity(group, embed_func):\n",
    "    test_texts = group['test_deepfake_analysis'].astype(str).tolist()\n",
    "    gt_texts = group['GT'].astype(str).tolist()\n",
    "    \n",
    "    if not test_texts or not gt_texts:\n",
    "        return 0.0\n",
    "    \n",
    "    test_embeddings = [embed_func(text) for text in test_texts]\n",
    "    gt_embeddings = [embed_func(text) for text in gt_texts]\n",
    "    \n",
    "    # Calculate cosine similarities for each pair of test and ground truth text\n",
    "    cosine_similarities = [\n",
    "        calculate_cosine_similarity(test_emb.reshape(1, -1), gt_emb.reshape(1, -1))\n",
    "        for test_emb, gt_emb in zip(test_embeddings, gt_embeddings)\n",
    "    ]\n",
    "    \n",
    "    # Return the average cosine similarity for the video group\n",
    "    return sum(cosine_similarities) / len(cosine_similarities) if cosine_similarities else 0.0\n",
    "\n",
    "# Calculate average cosine similarity per video using BERT\n",
    "bert_similarities = [\n",
    "    calculate_average_cosine_similarity(group, get_bert_embedding)\n",
    "    for _, group in video_groups\n",
    "]\n",
    "average_cosine_similarity_bert = sum(bert_similarities) / len(bert_similarities) if bert_similarities else 0.0\n",
    "print(f'Average Cosine Similarity (BERT): {average_cosine_similarity_bert}')\n",
    "\n",
    "# Calculate average cosine similarity per video using Sentence BERT\n",
    "sbert_similarities = []\n",
    "for _, group in video_groups:\n",
    "    test_texts = group['test_deepfake_analysis'].astype(str).tolist()\n",
    "    gt_texts = group['GT'].astype(str).tolist()\n",
    "    \n",
    "    if not test_texts or not gt_texts:\n",
    "        sbert_similarities.append(0.0)\n",
    "        continue\n",
    "    \n",
    "    # Get embeddings for both columns using Sentence BERT\n",
    "    test_embeddings_sbert = get_sbert_embedding(test_texts)\n",
    "    gt_embeddings_sbert = get_sbert_embedding(gt_texts)\n",
    "    \n",
    "    # Calculate cosine similarities for each pair of test and ground truth text\n",
    "    cosine_similarities_sbert = [\n",
    "        calculate_cosine_similarity(test_emb.cpu().unsqueeze(0).numpy(), gt_emb.cpu().unsqueeze(0).numpy())\n",
    "        for test_emb, gt_emb in zip(test_embeddings_sbert, gt_embeddings_sbert)\n",
    "    ]\n",
    "    \n",
    "    sbert_similarities.append(sum(cosine_similarities_sbert) / len(cosine_similarities_sbert) if cosine_similarities_sbert else 0.0)\n",
    "\n",
    "average_cosine_similarity_sbert = sum(sbert_similarities) / len(sbert_similarities) if sbert_similarities else 0.0\n",
    "print(f'Average Cosine Similarity (Sentence BERT): {average_cosine_similarity_sbert}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
