{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec0e9b-9e40-428a-9d86-12cf62d0b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('LLaVA')\n",
    "sys.path.append('LLaVA/llava')\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from pathlib import Path\n",
    "import json\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model_frame\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Base folder where video files are stored.\n",
    "BASE_DATA_DIR = Path('data')\n",
    "\n",
    "def normalize_dataset(ds):\n",
    "    ds_lower = ds.strip().lower()\n",
    "    mapping = {\n",
    "        \"deeperforensics\": \"Deeperforensics\",\n",
    "        \"deepfakedetection\": \"DeepfakeDetection\",\n",
    "        \"faceforensics++\": \"Faceforensics++\",\n",
    "        \"farceforensics++\": \"Faceforensics++\",  # fix misspelling\n",
    "        \"original\": \"Original\",\n",
    "    }\n",
    "    return mapping.get(ds_lower, ds)\n",
    "\n",
    "def find_movie_file_by_dataset_and_manipulation(movie_name, dataset_value, manipulation, base_folder):\n",
    "    \"\"\"\n",
    "    Search for a file whose name contains movie_name (case insensitive)\n",
    "    in the folder base_folder/normalized_dataset_value and ensure that one of its\n",
    "    parent folder names contains the manipulation string (case insensitive).\n",
    "    \"\"\"\n",
    "    normalized_ds = normalize_dataset(dataset_value)\n",
    "    target_dir = Path(base_folder) / normalized_ds\n",
    "    if not target_dir.exists():\n",
    "        print(f\"Dataset folder {target_dir} not found.\")\n",
    "        return None\n",
    "    for file_path in target_dir.rglob('*'):\n",
    "        if file_path.is_file() and movie_name.lower() in file_path.name.lower():\n",
    "            if manipulation.lower() in str(file_path.parent).lower():\n",
    "                return file_path.resolve()\n",
    "    return None\n",
    "\n",
    "class MaskUtils:\n",
    "    @staticmethod\n",
    "    def apply_mask(frame, keypoint, use_hard_mask=True, radius=75):\n",
    "        h, w, c = frame.shape\n",
    "        kp_x = keypoint[\"x\"] * w\n",
    "        kp_y = keypoint[\"y\"] * h\n",
    "        yy, xx = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')\n",
    "        distance = np.sqrt((xx - kp_x)**2 + (yy - kp_y)**2)\n",
    "        mask = (distance <= radius).astype(np.uint8)\n",
    "        if use_hard_mask:\n",
    "            for ch in range(c):\n",
    "                frame[:, :, ch] *= mask\n",
    "        else:\n",
    "            blur_ksize = 83\n",
    "            mask = mask.astype(np.float32)\n",
    "            blurred = cv2.GaussianBlur(mask, (blur_ksize, blur_ksize), 83)\n",
    "            blurred_normalized = cv2.normalize(blurred, None, 0, 1, cv2.NORM_MINMAX)\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "            dilated = cv2.dilate(blurred_normalized, kernel, iterations=1)\n",
    "            for ch in range(c):\n",
    "                frame[:, :, ch] = frame[:, :, ch].astype(np.float32) * dilated\n",
    "            frame = frame.astype(np.uint8)\n",
    "        return frame\n",
    "\n",
    "def safe_load_model(model_path, max_retries=3, delay=5):\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            print(f\"Loading model from {model_path} (attempt {attempt+1})...\")\n",
    "            tokenizer, llava_model, image_processor, context_len = load_pretrained_model(\n",
    "                model_path=model_path,\n",
    "                model_base=None,\n",
    "                model_name=get_model_name_from_path(model_path)\n",
    "            )\n",
    "            print(\"Model loaded.\")\n",
    "            return tokenizer, llava_model, image_processor, context_len\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "            attempt += 1\n",
    "    raise RuntimeError(\"Failed to load model.\")\n",
    "\n",
    "def get_clip_embedding(image, layer=\"last\"):\n",
    "    image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "    image_tensor = image_tensor.half() if clip_model.visual.conv1.weight.dtype == torch.half else image_tensor\n",
    "    with torch.no_grad():\n",
    "        if layer == \"first\":\n",
    "            x = clip_model.visual.conv1(image_tensor)\n",
    "            x = clip_model.visual.bn1(x)\n",
    "            x = clip_model.visual.relu1(x)\n",
    "            x = clip_model.visual.conv2(x)\n",
    "            x = clip_model.visual.bn2(x)\n",
    "            x = clip_model.visual.relu2(x)\n",
    "            x = clip_model.visual.conv3(x)\n",
    "            x = clip_model.visual.bn3(x)\n",
    "            x = clip_model.visual.relu3(x)\n",
    "            # Use adaptive average pooling to reduce the spatial dimensions to 1x1\n",
    "            x = torch.nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "            embedding = x.view(x.size(0), -1)\n",
    "\n",
    "        elif layer == \"middle\":\n",
    "            # Process through early layers\n",
    "            x = clip_model.visual.conv1(image_tensor)\n",
    "            x = clip_model.visual.bn1(x)\n",
    "            x = clip_model.visual.relu1(x)\n",
    "            x = clip_model.visual.conv2(x)\n",
    "            x = clip_model.visual.bn2(x)\n",
    "            x = clip_model.visual.relu2(x)\n",
    "            x = clip_model.visual.conv3(x)\n",
    "            x = clip_model.visual.bn3(x)\n",
    "            x = clip_model.visual.relu3(x)\n",
    "            x = clip_model.visual.avgpool(x)\n",
    "            # Process through later layers\n",
    "            x = clip_model.visual.layer1(x)\n",
    "            x = clip_model.visual.layer2(x)\n",
    "            x = clip_model.visual.layer3(x)\n",
    "            # Use adaptive average pooling to reduce spatial dimensions to 1x1\n",
    "            x = torch.nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "            embedding = x.view(x.size(0), -1)\n",
    "\n",
    "        elif layer == \"last\":\n",
    "            embedding = clip_model.encode_image(image_tensor)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layer specified.\")\n",
    "        embedding = embedding.view(embedding.size(0), -1)\n",
    "    return embedding\n",
    "\n",
    "def create_custom_prompt(annotations, prompt_version=1):\n",
    "    if prompt_version == 1:\n",
    "        base_prompt = (\n",
    "            \"Based on the following descriptions: {annotations}, analyze the face in the image and \"\n",
    "            \"identify any signs of deepfake artifacts. Provide a detailed description of any anomalies.\"\n",
    "        )\n",
    "    elif prompt_version == 2:\n",
    "        base_prompt = (\n",
    "            \"Analyze the face based on: '{annotations}'. Provide a short explanation highlighting inconsistencies.\"\n",
    "        )\n",
    "    elif prompt_version == 3:\n",
    "        base_prompt = (\n",
    "            \"Based on the annotations: {annotations}, examine the face for any signs of deepfake manipulation. \"\n",
    "            \"The provided annotations should serve as possible signs, but remember that the image can also be real. \"\n",
    "            \"Keep your answer between 10 and 30 words.\"\n",
    "        )\n",
    "    elif prompt_version == 4:\n",
    "        base_prompt = (\n",
    "            \"Analyze the face in the image based on the description: '{annotations}'. Identify any deepfake artifacts, focusing \"\n",
    "            \"specifically on the affected parts of the face mentioned. Provide a short and direct explanation,\"\n",
    "            \"highlighting the inconsistencies or manipulations.\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompt version.\")\n",
    "    return base_prompt.format(annotations=', '.join(f'\"{ann}\"' for ann in annotations))\n",
    "\n",
    "def detect_deepfake(frame, custom_prompt):\n",
    "    if not isinstance(frame, np.ndarray):\n",
    "        raise ValueError(\"Expected NumPy array.\")\n",
    "    frame = frame.astype('uint8')\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_pil = Image.fromarray(frame_rgb)\n",
    "    local_args = type('Args', (), {\n",
    "        \"model_path\": model_path,\n",
    "        \"model_base\": None,\n",
    "        \"model_name\": get_model_name_from_path(model_path),\n",
    "        \"query\": custom_prompt,\n",
    "        \"conv_mode\": None,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": None,\n",
    "        \"num_beams\": 1,\n",
    "        \"max_new_tokens\": 512,\n",
    "    })()\n",
    "    return eval_model_frame(\n",
    "        local_args,\n",
    "        image_pil,\n",
    "        tokenizer=tokenizer,\n",
    "        model=llava_model,\n",
    "        image_processor=image_processor\n",
    "    )\n",
    "\n",
    "def get_training_frames(df):\n",
    "    print(\"Processing training frames...\")\n",
    "    training_frames = {}\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Training rows\"):\n",
    "        if 'movie_path' in row and pd.notna(row['movie_path']):\n",
    "            video_path = Path(row['movie_path']).resolve()\n",
    "        else:\n",
    "            movie_name = row['movie_name']\n",
    "            dataset_val = row['dataset'] if 'dataset' in row and pd.notna(row['dataset']) else \"\"\n",
    "            manipulation = row['manipulation'] if 'manipulation' in row and pd.notna(row['manipulation']) else \"\"\n",
    "            video_path = find_movie_file_by_dataset_and_manipulation(movie_name, dataset_val, manipulation, BASE_DATA_DIR)\n",
    "            if video_path is None:\n",
    "                print(f\"[Row {idx}] File for '{movie_name}' with dataset '{dataset_val}' and manipulation '{manipulation}' not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "        click_locations = row['click_locations']\n",
    "        if not click_locations or pd.isna(click_locations):\n",
    "            continue\n",
    "        try:\n",
    "            frame_data = json.loads(click_locations)\n",
    "            # For training frames, store the annotation from \"text\" field\n",
    "            if (len(frame_data) == 1 and \"0\" in frame_data and \n",
    "                float(frame_data[\"0\"].get(\"x\", -1)) == 0.0 and float(frame_data[\"0\"].get(\"y\", -1)) == 0.0):\n",
    "                training_frames.setdefault(video_path, []).append((0, row['text']))\n",
    "            else:\n",
    "                for frame_str, _ in frame_data.items():\n",
    "                    if frame_str.isdigit():\n",
    "                        frame_num = int(frame_str) - 1\n",
    "                        training_frames.setdefault(video_path, []).append((frame_num, row['text']))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            continue\n",
    "    for vp, frames in training_frames.items():\n",
    "        training_frames[vp] = sorted(frames, key=lambda x: x[0])\n",
    "    print(f\"Processed training frames for {len(training_frames)} videos.\")\n",
    "    return training_frames\n",
    "\n",
    "def get_test_frames(df):\n",
    "\n",
    "    frames_dict = {}\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if 'movie_path' in row and pd.notna(row['movie_path']):\n",
    "            video_path = Path(row['movie_path']).resolve()\n",
    "        else:\n",
    "            movie_name = row['movie_name']\n",
    "            dataset_val = row['dataset'] if 'dataset' in row and pd.notna(row['dataset']) else \"\"\n",
    "            manipulation = row['manipulation'] if 'manipulation' in row and pd.notna(row['manipulation']) else \"\"\n",
    "            video_path = find_movie_file_by_dataset_and_manipulation(movie_name, dataset_val, manipulation, BASE_DATA_DIR)\n",
    "            if video_path is None:\n",
    "                continue\n",
    "\n",
    "        if not video_path.exists():\n",
    "            print(f\"Video not found at: {video_path}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Could not open video: {video_path}\")\n",
    "            continue\n",
    "        \n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        middle_frame = frame_count // 2\n",
    "        cap.release()\n",
    "        \n",
    "        ground_truth = row['text'] if 'text' in row and pd.notna(row['text']) else None\n",
    "        frames_dict[video_path] = [(middle_frame, [], ground_truth)]\n",
    "    \n",
    "    print(f\"Prepared middle-frame test data for {len(frames_dict)} videos.\")\n",
    "    return frames_dict\n",
    "\n",
    "def run_pipeline(rn_model_name, extraction_layer, top_k,\n",
    "                 mask_on=False, use_hard_mask=True, prompt_version=1):\n",
    "    print(f\"\\nLoading CLIP model {rn_model_name} on {device} ...\")\n",
    "    global clip_model, preprocess\n",
    "    clip_model, preprocess = clip.load(rn_model_name, device=device)\n",
    "    clip_model.eval()\n",
    "    print(\"CLIP model loaded.\")\n",
    "\n",
    "    print(\"Extracting training frames...\")\n",
    "    train_frames = get_training_frames(train_df)\n",
    "\n",
    "    print(\"Extracting test frames (middle only)...\")\n",
    "    test_frames = get_test_frames(test_df)\n",
    "\n",
    "    emb_save_path = f\"training_embeddings_{rn_model_name}_{extraction_layer}.pt\"\n",
    "    if os.path.exists(emb_save_path):\n",
    "        print(f\"Loading cached training embeddings from {emb_save_path} ...\")\n",
    "        emb_data = torch.load(emb_save_path, map_location=device)\n",
    "        training_embeddings_tensor = emb_data['embeddings']\n",
    "        training_annotations = emb_data['annotations']\n",
    "        training_keys = emb_data['keys']\n",
    "    else:\n",
    "        print(f\"Computing training embeddings for [{rn_model_name}, {extraction_layer}] ...\")\n",
    "        training_embeddings_list, training_annotations, training_keys = [], [], []\n",
    "        for video_path, frames in tqdm(train_frames.items(),\n",
    "                                       desc=f\"Processing training videos [{rn_model_name}, {extraction_layer}]\"):\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Could not open video: {video_path}\")\n",
    "                continue\n",
    "            for (frame_number, annotation) in frames:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image_pil = Image.fromarray(frame_rgb)\n",
    "                embedding = get_clip_embedding(image_pil, layer=extraction_layer)\n",
    "                training_embeddings_list.append(embedding)\n",
    "                training_annotations.append(annotation)\n",
    "                training_keys.append((video_path, frame_number))\n",
    "            cap.release()\n",
    "        if not training_embeddings_list:\n",
    "            raise RuntimeError(\"No training embeddings computed.\")\n",
    "        training_embeddings_tensor = torch.cat(training_embeddings_list, dim=0)\n",
    "        torch.save({\n",
    "            'embeddings': training_embeddings_tensor,\n",
    "            'annotations': training_annotations,\n",
    "            'keys': training_keys\n",
    "        }, emb_save_path)\n",
    "        print(f\"Saved training embeddings to {emb_save_path}\")\n",
    "\n",
    "    print(\"Analyzing test videos (middle frame)...\")\n",
    "    results = []\n",
    "    for video_path, frame_info_list in tqdm(test_frames.items(),\n",
    "                                            desc=f\"Analyzing test videos [{rn_model_name}, {extraction_layer}, top_k={top_k}]\"):\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Could not open video: {video_path}\")\n",
    "            continue\n",
    "\n",
    "        for (frame_number, keypoints, ground_truth) in frame_info_list:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image_pil = Image.fromarray(frame_rgb)\n",
    "            test_embedding = get_clip_embedding(image_pil, layer=extraction_layer)\n",
    "\n",
    "            sims = cosine_similarity(test_embedding, training_embeddings_tensor).squeeze(0)\n",
    "            sorted_indices = torch.argsort(sims, descending=True)\n",
    "\n",
    "            # Distinct videos among top matches\n",
    "            distinct_indices = []\n",
    "            used_videos = set()\n",
    "            for idx in sorted_indices.tolist():\n",
    "                vid_path, _ = training_keys[idx]\n",
    "                if vid_path not in used_videos:\n",
    "                    used_videos.add(vid_path)\n",
    "                    distinct_indices.append(idx)\n",
    "                if len(distinct_indices) == top_k:\n",
    "                    break\n",
    "\n",
    "            chosen_indices = distinct_indices\n",
    "            chosen_values = sims[chosen_indices]\n",
    "            chosen_annotations = [training_annotations[i] for i in chosen_indices]\n",
    "\n",
    "            custom_prompt = create_custom_prompt(chosen_annotations, prompt_version=prompt_version) \\\n",
    "                if chosen_annotations else \"No annotation available\"\n",
    "\n",
    "            frame_for_llava = frame.copy()\n",
    "            if mask_on and keypoints:\n",
    "                for kp in keypoints:\n",
    "                    frame_for_llava = MaskUtils.apply_mask(frame_for_llava, kp,\n",
    "                                                           use_hard_mask=use_hard_mask,\n",
    "                                                           radius=75)\n",
    "\n",
    "            test_deepfake_analysis = detect_deepfake(frame_for_llava, custom_prompt) \\\n",
    "                if chosen_annotations else \"No analysis available\"\n",
    "\n",
    "            results.append({\n",
    "                'rn_model': rn_model_name,\n",
    "                'extraction_layer': extraction_layer,\n",
    "                'top_k': top_k,\n",
    "                'test_video': str(video_path),\n",
    "                'test_frame': frame_number,\n",
    "                'ground_truth': ground_truth,\n",
    "                'closest_train_annotations': chosen_annotations,\n",
    "                'test_deepfake_analysis': test_deepfake_analysis,\n",
    "                'top_k_similarities': chosen_values.tolist(),\n",
    "                'prompt_version_used': prompt_version,\n",
    "            })\n",
    "        cap.release()\n",
    "    print(f\"Completed analysis for {len(results)} test frames.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting pipeline execution...\")\n",
    "\n",
    "    # Model path for LLaVA\n",
    "    model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "    tokenizer, llava_model, image_processor, context_len = safe_load_model(model_path)\n",
    "    llava_model = llava_model.to(device)\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    csv_path = 'dataset_last.csv'\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    \n",
    "    global train_df, val_df, test_df\n",
    "    train_df = df[df['split'] == 'train']\n",
    "    val_df = df[df['split'] == 'val']   # Not used in this script, but kept for reference\n",
    "    test_df = df[df['split'] == 'test']\n",
    "    \n",
    "    print(\"Dataset Summary:\")\n",
    "    train_unique = train_df[['dataset', 'manipulation', 'movie_name']].drop_duplicates().shape[0]\n",
    "    val_unique = val_df[['dataset', 'manipulation', 'movie_name']].drop_duplicates().shape[0]\n",
    "    test_unique = test_df[['dataset', 'manipulation', 'movie_name']].drop_duplicates().shape[0]\n",
    "    print(\"Total Train Videos:\", train_unique)\n",
    "    print(\"Total Validation Videos:\", val_unique)\n",
    "    print(\"Total Test Videos:\", test_unique)\n",
    "    \n",
    "    print(\"\\nRepresentative file paths per dataset and manipulation:\")\n",
    "    unique_ds_manip = df[['dataset', 'manipulation']].drop_duplicates()\n",
    "    for ds, manip in unique_ds_manip.values:\n",
    "        subset = df[(df['dataset'] == ds) & (df['manipulation'] == manip)]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        sample = subset.iloc[0]\n",
    "        movie_name = sample['movie_name']\n",
    "        if 'movie_path' in sample and pd.notna(sample['movie_path']):\n",
    "            path = Path(sample['movie_path']).resolve()\n",
    "        else:\n",
    "            path = find_movie_file_by_dataset_and_manipulation(movie_name, ds, manip, BASE_DATA_DIR)\n",
    "        print(f\"Dataset: {ds} | Manipulation: {manip} -> {path}\")\n",
    "    \n",
    "    # Preview how many frames are in the train/test sets\n",
    "    train_frames_info = get_training_frames(train_df)\n",
    "    test_frames_info = get_test_frames(test_df)\n",
    "    total_train_frames = sum(len(frames) for frames in train_frames_info.values())\n",
    "    total_test_frames = sum(len(frames) for frames in test_frames_info.values())\n",
    "    print(\"Total training frames of interest:\", total_train_frames)\n",
    "    print(\"Total test frames of interest:\", total_test_frames)\n",
    "    \n",
    "    # Example combos\n",
    "    rn_models = [\"RN101\"]\n",
    "    extraction_layers = [\"last\"]\n",
    "    top_k_values = [5]\n",
    "    mask_on_val = True\n",
    "    use_hard_mask_val = False\n",
    "    \n",
    "    all_results = []\n",
    "    for rn_model_name in rn_models:\n",
    "        for extraction_layer in extraction_layers:\n",
    "            for top_k in top_k_values:\n",
    "                print(f\"\\n=== Running: {rn_model_name}, {extraction_layer} extraction, top_k = {top_k} ===\")\n",
    "                comb_results = run_pipeline(\n",
    "                    rn_model_name=rn_model_name,\n",
    "                    extraction_layer=extraction_layer,\n",
    "                    top_k=top_k,\n",
    "                    mask_on=mask_on_val,\n",
    "                    use_hard_mask=use_hard_mask_val,\n",
    "                    prompt_version=4\n",
    "                )\n",
    "                comb_csv_path = f\"results_{rn_model_name}_{extraction_layer}_k{top_k}_mask_{mask_on_val}_hardmask_{use_hard_mask_val}.csv\"\n",
    "                pd.DataFrame(comb_results).to_csv(comb_csv_path, index=False)\n",
    "                print(f\"Saved results to {comb_csv_path}\")\n",
    "                all_results.extend(comb_results)\n",
    "    \n",
    "    combined_csv_path = \"results_all_combinations.csv\"\n",
    "    pd.DataFrame(all_results).to_csv(combined_csv_path, index=False)\n",
    "    print(f\"\\nAll results saved to {combined_csv_path}\")\n",
    "    print(\"Pipeline execution completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605cf46d-0c62-481e-bc8a-729733e81aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
